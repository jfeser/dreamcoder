from dreamcoder.utilities import *
from dreamcoder.recognition import *
from dreamcoder.enumeration import *
from dreamcoder.dreamcoder import ecIterator, default_wake_generative

from dreamcoder.domains.clevr.clevrPrimitives import *
from dreamcoder.domains.clevr.makeClevrTasks import *
import dreamcoder.domains.clevr.test_makeClevrTasks as test_makeClevrTasks

import os
import datetime
import random

"""
main.py (CLEVR) | Author: Catherine Wong
This is the main file for the CLEVR-based symbolic scene reasoning domain. It contains the domain-specific feature extractor and launch arguments for the CLEVR domain (though it is usually launched via the bin/clevr.py convenience file).

This dataset requires data generated by the Extended CLEVR dataset (https://github.com/CatherineWong/too_clevr). 

Example usage: TODO
"""

class ClevrFeatureExtractor(RecurrentFeatureExtractor):
    """
    Domain-specific recognition model for the CLEVR scene tasks.
    This also contains domain-specific flags used for serializing the tasks themselves.
    """
    
    special = 'clevr' # Special flag passed to the OCaml solver.
    serialize_special = serialize_clevr_object # Special flag passed to the serializer for the OCaml solver.
    maximum_helmholtz = 20000 
    
    def __init__(self, tasks, testingTasks=[], cuda=False):
        self.recomputeTasks = True
        self.useTask = True
        self.max_examples = 2
        self.max_examples_length = 200
        
        lexicon = clevr_lexicon() + ["OBJ_START", "OBJ_END"]
        super(ClevrFeatureExtractor, self).__init__(lexicon=lexicon,
                                                      H=64,
                                                      tasks=tasks,
                                                      bidirectional=True,
                                                      cuda=cuda,
                                                      helmholtzTimeout=0.5,
                                                      helmholtzEvaluationTimeout=0.25)
                                                      
    def tokenize(self, task):
        tokenized = []
        return_type, _ = infer_return_type([task.examples[0][-1]])
        def tokenize_obj_list(obj_list):
            flattened = []
            for o in obj_list:
                o_attrs = []
                for k, v in o.items():
                    if k in ["id", "color", "shape", "size", "material"]:
                        o_attrs += [k, str(v)]
                    else: # Relations
                        o_attrs += [k] + [str(rel) for rel in v]
                flattened += ["OBJ_START"] + o_attrs + ["OBJ_END"]
            return flattened
        def blank_task():
            blank_xs = (["OBJ_START", "OBJ_END"],)
            blank_y = ["OBJ_START", "OBJ_END"]
            return (blank_xs, blank_y)
        
        def example_len(xs, y):
            return sum([len(x) for x in xs]) + len(y)
        
        # To limit recognition backward pass times, we cap the length of these inputs
        for xs, y in task.examples:
            xs = [tokenize_obj_list(obj_list) for obj_list in xs]
            if return_type in [tint, tbool, tclevrsize, tclevrcolor, tclevrshape, tclevrsize, tclevrmaterial]:
                y = [str(y)]
            else:
                y = tokenize_obj_list(y)
            tokenized.append([xs, y])
        sorted_tokenized = sorted(tokenized, key = lambda e: example_len(e[0], e[1]))
        
        sorted_tokenized = [e if example_len(e[0], e[1]) <= self.max_examples_length else blank_task() for e in sorted_tokenized]
        tokenized = sorted_tokenized[:self.max_examples]
        return tokenized

    def taskOfProgram(self, p, tp):
        # Uses the RNN random input sampling
        t = super(ClevrFeatureExtractor, self).taskOfProgram(p, tp)
        if t is not None:
            xs, y = t.examples[0]
            if type(y) == list: # Sort and dedup any object list
                t.examples = [(xs, sort_and_dedup_obj_list(y)) for xs, y in t.examples]
        return t

all_train_questions = [
    "1_zero_hop",
    '1_one_hop',
    '1_compare_integer',
    '1_same_relate',
    '1_single_or',
    '2_remove',
    '2_transform'
    '2_localization'
]

def clevr_options(parser):
    ### Dataset loading options.
    parser.add_argument("--curriculumDatasets", type=str, nargs="*",
                        default=[],
                        help="A list of curriculum datasets, stored as JSON CLEVR question files. These will be run through separately.")
    parser.add_argument("--taskDatasets", type=str, nargs="*",
                        help="Which task datasets to load, stored as JSON CLEVR question files, or 'all' to load all of the datasets in the directory.")
    parser.add_argument("--taskDatasetDir",
                        default="data/clevr",
                        help="Top level directory for the dataset.")
    parser.add_argument("--languageDatasetDir",
                        default="data/clevr/language/")
                        
    # Primitive loading options.
    parser.add_argument("--primitives",
                        nargs="*",
                        default=["clevr_bootstrap", "clevr_map_transform"],
                        help="Which primitives to use. Choose from: [clevr_original, clevr_bootstrap, clevr_map_transform, clevr_filter, clevr_filter_except, clevr_difference]")

    parser.add_argument("--iterations_as_epochs",
                        default=True,
                        help="Whether to take the iterations value as an epochs value.")
    
    # Test functionalities.
    parser.add_argument("--run_makeClevrTasks_test",
                        action='store_true')
    parser.add_argument("--run_python_test",
                        action='store_true')
    parser.add_argument("--generate_ocaml_definitions",
                        action='store_true')
    parser.add_argument("--run_ocaml_test",
                        action='store_true')
    parser.add_argument("--run_recognition_test",
                        action='store_true')

def run_tests(args):
    if args.pop("run_makeClevrTasks_test"):
        test_makeClevrTasks.test_all()
        assert False
    pass    
                        
def main(args):
    # Entrypoint for running any tasks.
    run_tests(args)

    # Load the curriculum and datasets.
    # TODO: pop off all the unnecessary arguments.
    curriculum_datasets = args.pop("curriculumDatasets")
    task_dataset_dir=args.pop("taskDatasetDir")
    train_scenes, test_scenes = args.pop("trainInputScenes"), args.pop("testInputScenes")
    
    train, test = [], []
    if len(curriculum_datasets) > 0:
        curriculum, _ = loadCLEVRDataset(task_datasets=curriculum_datasets, task_dataset_dir=task_dataset_dir, train_scenes=train_scenes, test_scenes = test_scenes, seed=args["seed"], is_curriculum=True)
        train += curriculum
    
    task_datasets = args.pop("taskDatasets")
    train_tasks, test_tasks = loadCLEVRDataset(task_datasets=task_datasets, task_dataset_dir=task_dataset_dir, train_scenes=train_scenes, test_scenes = test_scenes, seed=args["seed"])
    train += train_tasks
    test += test_tasks
    
    eprint(f"Loaded datasets: [{task_datasets}]: [{len(train)}] total train and [{len(test)}] total test tasks. Using curriculum: [{curriculum_datasets}]")
    
    
    # Generate language dataset directly from the loaded tasks.
    args.pop("languageDataset")
    languageDataset = curriculum_datasets + task_datasets 
    if "curriculum" not in languageDataset:
        languageDataset += ["curriculum"] # To use the curriculum vocab.
    
    # Load the primitives and optionally run tests with the primitive set.
    primitive_names = args.pop("primitives")
    primitives = load_clevr_primitives(primitive_names)
    baseGrammar = Grammar.uniform(primitives)
    
    if args.pop("run_python_test"):
        run_clevr_primitives_test(primitive_names, curriculum)
        assert False
    
    if args.pop("generate_ocaml_definitions"):
        generate_ocaml_definitions(primitive_names)
        assert False
    
    if args.pop("run_ocaml_test"):
        # Test the Helmholtz enumeration
        # tasks = [buildClevrMockTask(train[0])]
        tasks = train[:10]
        if True:
            from dreamcoder.dreaming import backgroundHelmholtzEnumeration
            print(baseGrammar)
            helmholtzFrontiers = backgroundHelmholtzEnumeration(tasks, 
                                                                baseGrammar, 
                                                                timeout=5,
                                                                evaluationTimeout=0.05,
                                                                special='clevr',
                                                                executable='clevrTest',
                                                                serialize_special=serialize_clevr_object,
                                                                maximum_size=20000) # TODO: check if we need special to check tasks later
            f = helmholtzFrontiers()
            helmholtzFrontiers = backgroundHelmholtzEnumeration(train, 
                                                                baseGrammar, 
                                                                timeout=5,
                                                                evaluationTimeout=0.05,
                                                                special='clevr',
                                                                executable='helmholtz',
                                                                serialize_special=serialize_clevr_object,
                                                                maximum_size=20000) # TODO: check if we need special to check tasks later
            f = helmholtzFrontiers()
            assert False
        if False:
            # Check enumeration.
            tasks = [train[10]]
            default_wake_generative(baseGrammar, tasks, 
                                maximumFrontier=5,
                                enumerationTimeout=1,
                                CPUs=1,
                                solver='ocaml',
                                evaluationTimeout=0.05)
            assert False
    
    if args.pop("run_recognition_test"):
        tasks = [buildClevrMockTask(train[0])]
        featurizer = ClevrFeatureExtractor(tasks=tasks, testingTasks=[], cuda=False)
        for t in tasks:
            featurizer.featuresOfTask(t)
    
    
    use_epochs = args.pop("iterations_as_epochs")
    if use_epochs and args["taskBatchSize"] is not None:
        eprint("Using iterations as epochs")
        args["iterations"] *= int(len(train) / args["taskBatchSize"]) 
        eprint(f"Now running for n={args['iterations']} iterations.")

    timestamp = datetime.datetime.now().isoformat()
    # Escape the timestamp.
    timestamp = timestamp.replace(":", "-")
    timestamp = timestamp.replace(".", "-")
    outputDirectory = "experimentOutputs/clevr/%s"%timestamp
    os.system("mkdir -p %s"%outputDirectory)
    
    evaluationTimeout = 1.0
    print("Using starting grammar")
    print(baseGrammar)
    
    generator = ecIterator(baseGrammar, train,
                           testingTasks=test,
                           outputPrefix="%s/clevr"%outputDirectory,
                           evaluationTimeout=evaluationTimeout,
                           languageDataset=languageDataset,
                           **args)
    for result in generator:
        pass
        